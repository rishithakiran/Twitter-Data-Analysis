{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "Due Date: September 21, **BEFORE the beginning of class at 6:00pm**\n",
    "\n",
    "## **NOTE: There are *always* last minute issues submitting the case studies.  DO NOT WAIT UNTIL THE LAST MINUTE!**\n",
    "\n",
    "* ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Twitter_bird_logo_2012.svg/220px-Twitter_bird_logo_2012.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM 12 Members:** \n",
    "\n",
    "    Chu Wang\n",
    "    Saranya Manoharan\n",
    "    Rishitha Kiran\n",
    "    You Di\n",
    "    Valerie Tuzel\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x119e9dc88>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "def oauth_login():\n",
    "    # Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'Z7lkIGSYOQ7V2oz1V9Z2JSKnR'\n",
    "    CONSUMER_SECRET ='Wahjl2NWnkPpogbKw8JpPSTKkBTMX63yf0lYCfzSRwsOMgitOk'\n",
    "    #OAUTH_TOKEN = '571213367-jAEYPDKJy2T6PjrqmmAsxF6Bq1uJ8s0ADed1kqae'\n",
    "    OAUTH_TOKEN = '773982098256519173-38xCfCLocB5Pp0KQT0sPmk9wv728Kc5'\n",
    "    OAUTH_TOKEN_SECRET = 'TmhZcvgEf030QdrKe9yVjaZjzyKrQveJYwTmkKiDbHLSg'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "# displaying twitter_api to make sure that it's now a defined variable\n",
    "print(twitter_api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA Collection- DO NOT RUN AGAIN\n",
    "#Common Function to make Requests\n",
    "import sys\n",
    "import time\n",
    "#from urllib2 import URLError\n",
    "from urllib.error import URLError\n",
    "#from httplib import BadStatusLine\n",
    "from http.client import BadStatusLine\n",
    "import json\n",
    "import twitter\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
    "    \n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "    \n",
    "        if wait_period > 3600: # Seconds\n",
    "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
    "            raise e\n",
    "    \n",
    "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
    "    \n",
    "        if e.e.code == 401:\n",
    "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
    "            return None\n",
    "        elif e.e.code == 429: \n",
    "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
    "            if sleep_when_rate_limited:\n",
    "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
    "                (e.e.code, wait_period)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "    \n",
    "    wait_period = 2 \n",
    "    error_count = 0 \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except twitter.api.TwitterHTTPError as e:\n",
    "            error_count = 0 \n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise\n",
    "                \n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "#stream = twitter_stream.statuses.filter(track=q1 or q2 or q3 or q4 or q5 or q6)\n",
    "#q = 'Junk Food'\n",
    "q = 'ryanair'\n",
    "out_file_name_prefix = 'ual_'\n",
    "#q = 'iphone'\n",
    "#stream = twitter_stream.statuses.filter(track = q1)\n",
    "stream = make_twitter_request(twitter_stream.statuses.filter,\n",
    "                            track=q)\n",
    "\n",
    "# store the tweeets you downloaded into a json file(can not store the text without english)\n",
    "file_cnt = 1\n",
    "total_records = 0\n",
    "records_list = []\n",
    "try:\n",
    "    for tweet in stream:\n",
    "        total_records += 1\n",
    "        print('get records, total:', total_records)\n",
    "        records_list.append(tweet)\n",
    "        if total_records % 1000 == 0:\n",
    "            file_name = out_file_name_prefix + str(file_cnt) + '.data'\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(json.dumps(records_list, indent=4))\n",
    "            print('write %d records to file %s.....' % (len(records_list), file_name))\n",
    "            file_cnt += 1\n",
    "            records_list = []\n",
    "        if total_records == 10000:\n",
    "            break\n",
    "except KeyboardInterrupt as e:\n",
    "    print('process remain data')\n",
    "    if len(records_list) != 0:\n",
    "        file_name = out_file_name_prefix + str(file_cnt) + '.data'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json.dumps(records_list, indent=4))\n",
    "        print('last write %d records to file %s.....' % (len(records_list), file_name))\n",
    "    else:\n",
    "        print('no remain data')\n",
    "else:\n",
    "    print('success done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets collected:  6000 \n",
      "\n",
      "dict_keys(['created_at', 'id', 'id_str', 'text', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file = open('ryanair_6000.txt','r')\n",
    "ryanair = file.read()\n",
    "statuses = json.loads(ryanair)\n",
    "print(str(\"Total number of tweets collected: \"),len(statuses),\"\\n\")\n",
    "print(statuses[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report some statistics about the tweets you collected¶\n",
    "•The topic of interest: RYANAIR\n",
    "\n",
    "•The total number of tweets collected: 6000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(str.maketrans('','',string.punctuation))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/htuzel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "\n",
      "Top 30 Words With Their Counts: \n",
      "\n",
      "+---------------+-------+\n",
      "| Word          | Count |\n",
      "+---------------+-------+\n",
      "| cancelled     |   736 |\n",
      "| list          |   337 |\n",
      "| cancels       |   313 |\n",
      "| next          |   302 |\n",
      "| get           |   257 |\n",
      "| day           |   256 |\n",
      "| pilots        |   219 |\n",
      "| cancellations |   216 |\n",
      "| know          |   215 |\n",
      "| rights        |   185 |\n",
      "| leave         |   183 |\n",
      "| cancelling    |   180 |\n",
      "| people        |   173 |\n",
      "| dont          |   171 |\n",
      "| messing       |   170 |\n",
      "| days          |   167 |\n",
      "| weeks         |   166 |\n",
      "| entitled      |   159 |\n",
      "| full          |   158 |\n",
      "| losing        |   156 |\n",
      "| us            |   150 |\n",
      "| brexit        |   149 |\n",
      "| forget        |   147 |\n",
      "| workers       |   146 |\n",
      "| annual        |   141 |\n",
      "| customers     |   138 |\n",
      "| talk          |   138 |\n",
      "| going         |   127 |\n",
      "| please        |   121 |\n",
      "| one           |   119 |\n",
      "+---------------+-------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "tweet_texts = [ tweet['text'] \n",
    "                 for tweet in statuses\n",
    "                      if tweet[\"lang\"] == \"en\" ]\n",
    "words = [ word \n",
    "          for tweet in tweet_texts\n",
    "             for word in tweet.strip().split()\n",
    "                if word not in ['RT', '&amp;']  # filter out RT and ampersand\n",
    "        ]\n",
    "\n",
    "#clear = translator(delete=string.)\n",
    "#cleanwords = [w.clear() for w.strip() in words ]\n",
    "\n",
    "# Use the natural language toolkit to eliminate stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stop_words():\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words += [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%','🙄…']\n",
    "    stop_words += list(string.punctuation)\n",
    "    return stop_words \n",
    " \n",
    "hashtag_set = set()\n",
    "for tweet in statuses:\n",
    "    if \"entities\" in tweet.keys() and len(tweet[\"entities\"])!=0 :\n",
    "        if \"hashtags\" in tweet[\"entities\"] and len(tweet[\"entities\"][\"hashtags\"])!=0:\n",
    "            for hashtag in tweet[\"entities\"][\"hashtags\"]:\n",
    "                hashtag_set.add(hashtag[\"text\"])\n",
    "                #hashtag_list[hashtag[\"text\"]] = hashtag_list.get(hashtag[\"text\"])\n",
    "\n",
    "stop_words = stop_words()\n",
    "content_words = [w for w in words if not w.startswith(('https:','http','@','#','0','1','2','3','4','5','6','7','8','9')) ]\n",
    "clean_words = [ w.translate(str.maketrans('','',string.punctuation)) for w in content_words ]\n",
    "lower_words = [w.lower() for w in clean_words]\n",
    "pure_words = [w.strip() for w in lower_words if w.strip()!='']\n",
    "popular_words = [w for w in pure_words if w.lower() not in stop_words]\n",
    "non_stop_words = [w for w in popular_words if w.lower() not in hashtag_set]\n",
    "\n",
    "# frequency of words\n",
    "count = Counter(non_stop_words).most_common()\n",
    "\n",
    "# table of the top 30 words with their counts\n",
    "print(\"\\n\")\n",
    "print(str(\"Top 30 Words With Their Counts:\"),\"\\n\")\n",
    "\n",
    "pretty_table = PrettyTable(field_names=['Word', 'Count']) \n",
    "[ pretty_table.add_row(w) for w in count[:30] ]\n",
    "pretty_table.align['Word'] = 'l'\n",
    "pretty_table.align['Count'] = 'r'\n",
    "print(pretty_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find the most popular tweets in your collection of tweets\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Tweets: \n",
      "\n",
      "+-------+--------------------------------------------------------------+\n",
      "| Count | Text                                                         |\n",
      "+-------+--------------------------------------------------------------+\n",
      "| 1062  | RT @Ryanair: For a chance to WIN a €/£100 voucher simply     |\n",
      "|       | FOLLOW, RETWEET &amp; tell us which destination you stopped  |\n",
      "|       | on below using…                                              |\n",
      "| 434   | RT @Channel4Racing: VAUTOUR with @Ruby_Walsh on board finds  |\n",
      "|       | an extra gear to win the Ryanair Chase! #CheltenhamFestival  |\n",
      "|       | https://t.co/ezSqhc…                                         |\n",
      "| 136   | RT @LeaveEUOfficial: When you talk about losing workers'     |\n",
      "|       | rights after Brexit but forget your pilots are entitled to   |\n",
      "|       | annual leave... 🙄…                                           |\n",
      "| 135   | RT @PaulWoolford: How is it legal that @Ryanair can cancel   |\n",
      "|       | 400,000 seats on it's flights yet there is no way to cancel  |\n",
      "|       | a Ryanair seat &amp; get…                                    |\n",
      "| 124   | RT @MrHarryCole: While lecturing about politics.             |\n",
      "|       | https://t.co/OK5fXVIFZ5                                      |\n",
      "| 115   | RT @_chloemo: @Ryanair This isn't good enough. Release a     |\n",
      "|       | list now of all the flights you plan to cancel so your       |\n",
      "|       | paying customers…                                            |\n",
      "| 111   | RT @SimonCalder: Ryanair cancellations: up to 400,000        |\n",
      "|       | passengers affected. Your rights here. Let me know if you    |\n",
      "|       | have other questions.                                        |\n",
      "|       | https…                                                       |\n",
      "| 109   | RT @ollieoioioi: If your flight is cancelled by #ryanair DO  |\n",
      "|       | NOT accept the refund. You can sue them for compensation for |\n",
      "|       | not giving 2 weeks…                                          |\n",
      "| 108   | RT @BBCWorld: Ryanair cancels flights after 'messing up'     |\n",
      "|       | pilot holidays https://t.co/kfY5iUCBPq                       |\n",
      "| 104   | RT @patrick506: @Ryanair Publish a full list now of all the  |\n",
      "|       | flights you intend to cancel, save yourselves some goodwill, |\n",
      "|       | possibly. @Ryanair                                           |\n",
      "+-------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "#function to count the tweets with maximum retweets\n",
    "def popular_tweets_count(statuses):\n",
    "    popular = {}\n",
    "    for tweet in statuses:\n",
    "        if \"retweeted_status\" in tweet.keys() and len(tweet[\"retweeted_status\"])!=0\tand tweet[\"lang\"] == \"en\" \\\n",
    "            and \"retweet_count\" in tweet[\"retweeted_status\"].keys():\n",
    "                if tweet[\"retweeted_status\"][\"retweet_count\"] != 0 and \"text\" in tweet.keys() and len(tweet[\"text\"])!=0:\n",
    "                    #to make sure same tweet doesn't go into the list more than once\n",
    "                    if tweet[\"text\"] not in popular.values(): \n",
    "                        popular[tweet[\"retweeted_status\"][\"retweet_count\"]] = tweet[\"text\"]\n",
    "    # sort the most popular tweets and extract the top 10                    \n",
    "    sorted_popular = sorted(popular.items(), key=lambda t: t[0], reverse = True)[:10]\n",
    "    #return top 10 tweets\n",
    "    return sorted_popular\n",
    "\n",
    "#get the top 10 tweets\n",
    "popular_tweets_count = popular_tweets_count(statuses)\n",
    "#for tweet in sorted(popular_tweets_count, reverse=True):\n",
    "    #print(\"Retweeted_count: \", tweet[0], \"\\t\",\"Tweet_text: \", tweet[1])\n",
    "\n",
    "print(str(\"Top 10 Tweets:\"),\"\\n\")\n",
    "table = PrettyTable(field_names=['Count', 'Text'])\n",
    "[ table.add_row(row) for row in sorted (popular_tweets_count, reverse=True)]\n",
    "table.max_width['Text'] = 60\n",
    "table.align= 'l'\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------+\n",
      "|     Top 10 hashtags     | hashtags count |\n",
      "+-------------------------+----------------+\n",
      "|         Ryanair         |      487       |\n",
      "|   ryanaircancellations  |      175       |\n",
      "|         ryanair         |       80       |\n",
      "|       bustbyfriday      |       61       |\n",
      "|         gameover        |       56       |\n",
      "|          marianِ         |       38       |\n",
      "|           news          |       28       |\n",
      "|          travel         |       27       |\n",
      "|   RyanairCancellations  |       24       |\n",
      "| Ryanaircancelledflights |       22       |\n",
      "+-------------------------+----------------+ \n",
      "\n",
      "+----------------------+---------------------+\n",
      "| Top 10 user_mentions | user_mentions count |\n",
      "+----------------------+---------------------+\n",
      "|       Ryanair        |         2491        |\n",
      "|   LeaveEUOfficial    |         249         |\n",
      "|        facua         |         157         |\n",
      "|    controladores     |         133         |\n",
      "|       A4Europe       |          98         |\n",
      "|      Femi_Sorry      |          76         |\n",
      "|    MinutemanItaly    |          69         |\n",
      "|     Independent      |          67         |\n",
      "|   thecarolemalone    |          52         |\n",
      "|     SimonCalder      |          50         |\n",
      "+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#function to get the top 10 hashtags\n",
    "#check that the entities in a tweet are not empty, then check if it has hashtags\n",
    "#for each hash tag in a tweet add the hashtag and count into the list ,increment the count accordingly\n",
    "#sort the list of hashtags in decreasing order and extract the top 10 \n",
    "#return the top 10 hash tags\n",
    "def top_hashtag(statuses):\n",
    "    hashtag_list ={}\n",
    "    for tweet in statuses:\n",
    "        if \"entities\" in tweet.keys() and len(tweet[\"entities\"])!=0:\n",
    "            if \"hashtags\" in tweet[\"entities\"] and len(tweet[\"entities\"][\"hashtags\"])!=0:\n",
    "                for hashtag in tweet[\"entities\"][\"hashtags\"]:\n",
    "                    hashtag_list[hashtag[\"text\"]] = hashtag_list.get(hashtag[\"text\"], 0)+1\n",
    "    top10_hashtags = sorted(hashtag_list.items(), key=lambda t: t[1], reverse=True)[:10]\n",
    "    return top10_hashtags\n",
    "\n",
    "\n",
    "#function to get the top 10 user_mentions\n",
    "#check that the entities in a tweet are not empty, then check if it has user_mentions\n",
    "#for each user_mention in a tweet add the user_mention and count into the list ,increment the count acordingly\n",
    "#sort the list of user_mentions in dscending order and extract the top 10 \n",
    "#return the top 10 user_mentions\n",
    "def top_userMentions(statuses):\n",
    "    user_mentions_list= {}\n",
    "    for tweet in statuses:\n",
    "        if \"entities\" in tweet.keys() and len(tweet[\"entities\"])!=0:\n",
    "            if \"user_mentions\" in tweet[\"entities\"] and len(tweet[\"entities\"][\"user_mentions\"])!=0:\n",
    "                for user_mention in tweet[\"entities\"][\"user_mentions\"]:\n",
    "                    user_mentions_list[user_mention['screen_name']] = user_mentions_list.get(user_mention['screen_name'],0)+1\n",
    "    top10_user_mentions = sorted(user_mentions_list.items(), key=lambda t: t[1], reverse=True)[:10]\n",
    "    return top10_user_mentions\n",
    "\n",
    "\n",
    "#all the tweets\n",
    "content = statuses\n",
    "#call functions to get top 10 hash tags and user_mentions\n",
    "top_hashtags = top_hashtag(statuses)\n",
    "top_user_mentions = top_userMentions(statuses)\n",
    "\n",
    "#create table to display the top 10 hashtags\n",
    "hashtags_table = PrettyTable(field_names = ['Top 10 hashtags', 'hashtags count'])\n",
    "[ hashtags_table.add_row(row) for row in top_hashtags]\n",
    "#create table to display the top 10 user_mentions\n",
    "user_mentions_table = PrettyTable(field_names = ['Top 10 user_mentions', 'user_mentions count'])\n",
    "[ user_mentions_table.add_row(row) for row in top_user_mentions]\n",
    "\n",
    "#display the tables\n",
    "print(hashtags_table,\"\\n\")\n",
    "print(user_mentions_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ------------------------\n",
    "\n",
    "# Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"id\": 21447363,\n",
      "  \"id_str\": \"21447363\",\n",
      "  \"name\": \"KATY PERRY\",\n",
      "  \"screen_name\": \"katyperry\",\n",
      "  \"location\": \"\",\n",
      "  \"description\": \"\",\n",
      "  \"url\": \"https://t.co/UCVo8NkmAc\",\n",
      "  \"entities\": {\n",
      "   \"url\": {\n",
      "    \"urls\": [\n",
      "     {\n",
      "      \"url\": \"https://t.co/UCVo8NkmAc\",\n",
      "      \"expanded_url\": \"http://www.katyperry.com/tour\",\n",
      "      \"display_url\": \"katyperry.com/tour\",\n",
      "      \"indices\": [\n",
      "       0,\n",
      "       23\n",
      "      ]\n",
      "     }\n",
      "    ]\n",
      "   },\n",
      "   \"description\": {\n",
      "    \"urls\": []\n",
      "   }\n",
      "  },\n",
      "  \"protected\": false,\n",
      "  \"followers_count\": 104010102,\n",
      "  \"friends_count\": 205,\n",
      "  \"listed_count\": 141927,\n",
      "  \"created_at\": \"Fri Feb 20 23:45:56 +0000 2009\",\n",
      "  \"favourites_count\": 5761,\n",
      "  \"utc_offset\": -28800,\n",
      "  \"time_zone\": \"Alaska\",\n",
      "  \"geo_enabled\": true,\n",
      "  \"verified\": true,\n",
      "  \"statuses_count\": 8675,\n",
      "  \"lang\": \"en\",\n",
      "  \"status\": {\n",
      "   \"created_at\": \"Tue Sep 19 22:49:55 +0000 2017\",\n",
      "   \"id\": 910274784474300416,\n",
      "   \"id_str\": \"910274784474300416\",\n",
      "   \"text\": \"it's almost time tiny dancer #witnessthetour https://t.co/LtivMIrJBT\",\n",
      "   \"truncated\": false,\n",
      "   \"entities\": {\n",
      "    \"hashtags\": [\n",
      "     {\n",
      "      \"text\": \"witnessthetour\",\n",
      "      \"indices\": [\n",
      "       29,\n",
      "       44\n",
      "      ]\n",
      "     }\n",
      "    ],\n",
      "    \"symbols\": [],\n",
      "    \"user_mentions\": [],\n",
      "    \"urls\": [\n",
      "     {\n",
      "      \"url\": \"https://t.co/LtivMIrJBT\",\n",
      "      \"expanded_url\": \"https://www.instagram.com/p/BZPW6AtFntQ/\",\n",
      "      \"display_url\": \"instagram.com/p/BZPW6AtFntQ/\",\n",
      "      \"indices\": [\n",
      "       45,\n",
      "       68\n",
      "      ]\n",
      "     }\n",
      "    ]\n",
      "   },\n",
      "   \"source\": \"<a href=\\\"http://instagram.com\\\" rel=\\\"nofollow\\\">Instagram</a>\",\n",
      "   \"in_reply_to_status_id\": null,\n",
      "   \"in_reply_to_status_id_str\": null,\n",
      "   \"in_reply_to_user_id\": null,\n",
      "   \"in_reply_to_user_id_str\": null,\n",
      "   \"in_reply_to_screen_name\": null,\n",
      "   \"geo\": null,\n",
      "   \"coordinates\": null,\n",
      "   \"place\": null,\n",
      "   \"contributors\": null,\n",
      "   \"is_quote_status\": false,\n",
      "   \"retweet_count\": 1221,\n",
      "   \"favorite_count\": 4066,\n",
      "   \"favorited\": false,\n",
      "   \"retweeted\": false,\n",
      "   \"possibly_sensitive\": false,\n",
      "   \"lang\": \"en\"\n",
      "  },\n",
      "  \"contributors_enabled\": false,\n",
      "  \"is_translator\": false,\n",
      "  \"is_translation_enabled\": true,\n",
      "  \"profile_background_color\": \"CECFBC\",\n",
      "  \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/378800000168797027/kSZ-ewZo.jpeg\",\n",
      "  \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/378800000168797027/kSZ-ewZo.jpeg\",\n",
      "  \"profile_background_tile\": false,\n",
      "  \"profile_image_url\": \"http://pbs.twimg.com/profile_images/902653914465550336/QE3287ZJ_normal.jpg\",\n",
      "  \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/902653914465550336/QE3287ZJ_normal.jpg\",\n",
      "  \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/21447363/1504044451\",\n",
      "  \"profile_link_color\": \"D55732\",\n",
      "  \"profile_sidebar_border_color\": \"FFFFFF\",\n",
      "  \"profile_sidebar_fill_color\": \"78C0A8\",\n",
      "  \"profile_text_color\": \"5E412F\",\n",
      "  \"profile_use_background_image\": true,\n",
      "  \"has_extended_profile\": true,\n",
      "  \"default_profile\": false,\n",
      "  \"default_profile_image\": false,\n",
      "  \"following\": false,\n",
      "  \"follow_request_sent\": false,\n",
      "  \"notifications\": false,\n",
      "  \"translator_type\": \"regular\"\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Common Function to make Requests\n",
    "import sys\n",
    "import time\n",
    "#from urllib2 import URLError\n",
    "from urllib.error import URLError\n",
    "#from httplib import BadStatusLine\n",
    "from http.client import BadStatusLine\n",
    "import json\n",
    "import twitter\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
    "    \n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "    \n",
    "        if wait_period > 3600: # Seconds\n",
    "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
    "            raise e\n",
    "    \n",
    "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
    "    \n",
    "        if e.e.code == 401:\n",
    "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
    "            return None\n",
    "        elif e.e.code == 429: \n",
    "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
    "            if sleep_when_rate_limited:\n",
    "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
    "                (e.e.code, wait_period)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "    \n",
    "    wait_period = 2 \n",
    "    error_count = 0 \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except twitter.api.TwitterHTTPError as e:\n",
    "            error_count = 0 \n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/users/lookup for \n",
    "# twitter_api.users.lookup\n",
    "\n",
    "response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                screen_name=\"katyperry\")\n",
    "\n",
    "print(json.dumps(response, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA Collection- DO NOT RUN AGAIN\n",
    "import time\n",
    "from functools import partial\n",
    "maxint=20000\n",
    "\n",
    "def get_friends_followers_ids(twitter_api, screen_name=None, user_id=None,\n",
    "                              friends_limit=maxint, followers_limit=maxint):\n",
    "    \n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_name != None) != (user_id != None), \\\n",
    "    \"Must have screen_name or user_id, but not both\"\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/friends/ids and\n",
    "    # https://dev.twitter.com/docs/api/1.1/get/followers/ids for details\n",
    "    # on API parameters\n",
    "    \n",
    "    get_friends_ids = partial(make_twitter_request, twitter_api.friends.ids, \n",
    "                              count=5000)\n",
    "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids, \n",
    "                                count=5000)\n",
    "\n",
    "    friends_ids, followers_ids = [], []\n",
    "    \n",
    "    for twitter_api_func, limit, ids, label in [\n",
    "                    [get_friends_ids, friends_limit, friends_ids, \"friends\"], \n",
    "                    [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
    "                ]:\n",
    "        \n",
    "        if limit == 0: \n",
    "            continue\n",
    "        # initialize the file\n",
    "        with open(label + '.data', 'w') as f:\n",
    "            pass\n",
    "        \n",
    "        cursor = -1\n",
    "        while cursor != 0:\n",
    "        \n",
    "            # Use make_twitter_request via the partially bound callable...\n",
    "            if screen_name: \n",
    "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
    "            else: # user_id\n",
    "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
    "\n",
    "            if response is not None:\n",
    "                with open(label + '.data', 'a') as f:\n",
    "                    f.write('\\n'.join([str(item) for item in response['ids']]) + '\\n')\n",
    "                # if it gets too large, drop the id\n",
    "                # ids += response['ids']\n",
    "                print('cursor:',cursor, '...........................................................') \n",
    "                cursor = response['next_cursor']\n",
    "        \n",
    "            print >> sys.stderr, 'Fetched {0} total {1} ids for {2}'.format(len(ids), \n",
    "                                                    label, (user_id or screen_name))\n",
    "        \n",
    "            # XXX: You may want to store data during each iteration to provide an \n",
    "            # an additional layer of protection from exceptional circumstances\n",
    "        \n",
    "            if len(ids) >= limit or response is None:\n",
    "                #print len(ids), limit, 'break'\n",
    "                break\n",
    "            # control the speed\n",
    "            time.sleep(15)\n",
    "\n",
    "    # Do something useful with the IDs, like store them to disk...\n",
    "    return friends_ids[:friends_limit], followers_ids[:followers_limit]\n",
    "\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "friends_ids, followers_ids = get_friends_followers_ids(twitter_api, \n",
    "                                                       screen_name=\"katyperry\", \n",
    "                                                       friends_limit=10000, \n",
    "                                                       followers_limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 of Katy Perry's Friends: \n",
      "\n",
      "+--------------------+-----------------+\n",
      "| id                 |     screen_name |\n",
      "+--------------------+-----------------+\n",
      "| 11348282           |            NASA |\n",
      "| 346276932          |       TaraBrach |\n",
      "| 19725644           |       neiltyson |\n",
      "| 52544275           |     IvankaTrump |\n",
      "| 1652541            |         Reuters |\n",
      "| 51241574           |              AP |\n",
      "| 29450962           |    repjohnlewis |\n",
      "| 63302020           |     JordanPeele |\n",
      "| 181572333          | chancetherapper |\n",
      "| 757303975          |  ChelseaClinton |\n",
      "| 21288052           |        zanelowe |\n",
      "| 88975905           |    shanesmith30 |\n",
      "| 3327720838         |      brielarson |\n",
      "| 82455213           |          RuPaul |\n",
      "| 39364684           |   chrissyteigen |\n",
      "| 1923827275         |      SkipMarley |\n",
      "| 29417304           |           deray |\n",
      "| 531561605          |  AmericaFerrera |\n",
      "| 796974685288157184 |    iwillharness |\n",
      "| 784152188092190720 |   kpcollections |\n",
      "+--------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving user ids to screen names: 20/20\n"
     ]
    }
   ],
   "source": [
    "from twitter import follow\n",
    "\n",
    "# read the data from file, build a list, get top 20 and plot\n",
    "with open('friends.data') as f:\n",
    "    lines = f.readlines()\n",
    "friends_ids = [id_item.strip() for id_item in lines] \n",
    "#print(friends_ids)\n",
    "friends_dict = follow.lookup(twitter_api, user_ids=friends_ids[:20])\n",
    "\n",
    "#print(friends_dict)\n",
    "#followers_dict = follow.lookup(twitter_api, user_ids=followers_ids[:friends_limit])\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "pt = PrettyTable(field_names=['id', 'screen_name'])\n",
    "for friend_id in friends_dict:\n",
    "    screen_name = friends_dict[friend_id]\n",
    "    pt.add_row([friend_id, screen_name])\n",
    "    pt.align['id'], pt.align['screen_name'] = 'l', 'r' # Set column alignment\n",
    "print(str(\"20 of Katy Perry's Friends:\"),\"\\n\")\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 of Katy Perry's Followers: \n",
      "\n",
      "+--------------------+-----------------+\n",
      "| id                 |     screen_name |\n",
      "+--------------------+-----------------+\n",
      "| 908693273559961600 |      sartre6990 |\n",
      "| 908693184456347648 | ZubairC05824659 |\n",
      "| 908692829353926657 |      MissKagame |\n",
      "| 908693389213761536 |     Darmawan126 |\n",
      "| 908693198469582853 | gigicarpenter51 |\n",
      "| 908693353138548736 | EmilyLe21361259 |\n",
      "| 4619960298         |  purnomoarif313 |\n",
      "| 908692967359053825 | Virgini12060771 |\n",
      "| 908693190089175040 |  Crisel94082929 |\n",
      "| 886281281759215616 |    AlperCiftci8 |\n",
      "| 900599865096273920 |  MagarVaishnavi |\n",
      "| 906158287783174148 |      lisa128911 |\n",
      "| 1917952952         |     Tymeshionna |\n",
      "| 908692791210868736 |      DullaVicky |\n",
      "| 908693240655867904 | MiriamM61123558 |\n",
      "| 908693154932523009 | Sanjayj76299545 |\n",
      "| 908692677058813953 |  alejavelez2785 |\n",
      "| 908540579398639616 |     REBiRTH1027 |\n",
      "| 908690321009201153 |    LouLouOttawa |\n",
      "| 908692577335070720 | Y4gCR8ozDH9GNXs |\n",
      "+--------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving user ids to screen names: 20/20\n"
     ]
    }
   ],
   "source": [
    "from twitter import follow\n",
    "\n",
    "# read the data from file, build a list, get top 20 and plot\n",
    "with open('followers.data') as f:\n",
    "    lines = f.readlines()\n",
    "followers_ids = [id_item.strip() for id_item in lines] \n",
    "#print(friends_ids)\n",
    "followers_dict = follow.lookup(twitter_api, user_ids=followers_ids[:20])\n",
    "\n",
    "#print(friends_dict)\n",
    "#followers_dict = follow.lookup(twitter_api, user_ids=followers_ids[:friends_limit])\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "pt = PrettyTable(field_names=['id', 'screen_name'])\n",
    "for followers_id in followers_dict:\n",
    "    screen_name = followers_dict[followers_id]\n",
    "    pt.add_row([followers_id, screen_name])\n",
    "    pt.align['id'], pt.align['screen_name'] = 'l', 'r' # Set column alignment\n",
    "print(str(\"20 of Katy Perry's Followers:\"),\"\\n\")\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Friends: \n",
      "\n",
      "+----+-------------+\n",
      "| id | screen_name |\n",
      "+----+-------------+\n",
      "+----+-------------+\n"
     ]
    }
   ],
   "source": [
    "# build a list of all friends\n",
    "with open('friends.data') as f:\n",
    "    lines = f.readlines()\n",
    "firends_ids = [id_item.strip() for id_item in lines]\n",
    "\n",
    "# traverse 'followers' list，decide if it exists in 'friends' list\n",
    "f = open('followers.data')\n",
    "line = f.readline()\n",
    "common_ids = []\n",
    "while line:\n",
    "    follower_id = line.strip()\n",
    "    if follower_id in friends_ids:\n",
    "        common_ids.append(follower_id)\n",
    "    line = f.readline()\n",
    "\n",
    "common_dict = follow.lookup(twitter_api, user_ids=common_ids)\n",
    "#followers_dict = follow.lookup(twitter_api, user_ids=followers_ids[:friends_limit])\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "pt = PrettyTable(field_names=['id', 'screen_name'])\n",
    "for friend_id in friends_dict:\n",
    "    screen_name = friends_dict[friend_id]\n",
    "    pt.align['id'], pt.align['screen_name'] = 'l', 'r' # Set column alignment\n",
    "print(str(\"Mutual Friends:\"),\"\\n\")\n",
    "print(pt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "# Problem 4: Business question \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
    "\n",
    "* Come up with a business question that Twitter data could help answer.\n",
    "* Decribe the business case.\n",
    "* How could Twitter data help a company decide how to spend its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for draw pie plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generic_pie_plot(D, num_shown=5, include_others=True,\n",
    "                    startangle=90,shadow=True, autopct='%1.1f%%', \n",
    "                    xlabel=None, ylabel=None, title=None):\n",
    "    sorted_D = sorted(D.items(), key=lambda t: t[1], reverse=True)\n",
    "    print(sorted_D)\n",
    "    colors = ['#87ceeb','#ffa500','#ff4500','b','g']*5\n",
    "    colors = colors[:num_shown]\n",
    "    labels = []\n",
    "    slices = []\n",
    "    explode = [0.1]+[0]*(num_shown-1)\n",
    "    if include_others == True:\n",
    "        sum_total = 0\n",
    "        for pair in sorted_D:\n",
    "            sum_total += pair[1]\n",
    "        shown_total = 0\n",
    "        for pair in sorted_D[:num_shown-1]:\n",
    "            labels.append(pair[0])\n",
    "            slices.append(pair[1])\n",
    "            shown_total += pair[1]\n",
    "        labels.append('Others')\n",
    "        slices.append(sum_total-shown_total)\n",
    "    else:\n",
    "        for pair in sorted_D[:num_shown]:\n",
    "            labels.append(pair[0])\n",
    "            slices.append(pair[1])\n",
    "    plt.pie(slices,labels=labels,colors = colors,startangle = startangle, \n",
    "                shadow = True, explode = explode, autopct = autopct)\n",
    "    if xlabel==None and ylabel==None and title==None:\n",
    "        plt.show()\n",
    "    elif xlabel==None and ylabel==None:\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Saranya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Generate word libraries of positive words and negative words, respectively\n",
    "\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "negative_tweets_abs = twitter_samples.abspath(\"negative_tweets.json\")\n",
    "positive_tweets_abs = twitter_samples.abspath(\"positive_tweets.json\")\n",
    "negative_word_tokenized = twitter_samples.tokenized(negative_tweets_abs)\n",
    "positive_word_tokenized = twitter_samples.tokenized(positive_tweets_abs)\n",
    "\n",
    "def stop_words():\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words += [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "    stop_words += list(string.punctuation)\n",
    "    return stop_words \n",
    "unwanted_words_list = stop_words()\n",
    "\n",
    "def clean_up_word_set(ls):\n",
    "    word_set = []\n",
    "    for l in ls:\n",
    "        for word in l:\n",
    "            if word not in unwanted_words_list and word not in word_set and not word.startswith(('.','https:','http','@','#','0','1','2','3','4','5','6','7','8','9')):\n",
    "                    word_set.append(word)\n",
    "    return word_set\n",
    "\n",
    "negative_words = clean_up_word_set(negative_word_tokenized)\n",
    "positive_words = clean_up_word_set(positive_word_tokenized)\n",
    "\n",
    "with open(\"negative_words_cleaned.json\", \"w\", encoding='utf-8') as f_neg:\n",
    "    f_neg.write(json.dumps(negative_words, indent=3))\n",
    "\n",
    "with open(\"positive_words_cleaned.json\", \"w\", encoding='utf-8') as f_pos:\n",
    "    f_pos.write(json.dumps(positive_words, indent=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Saranya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[('negative', 1804), ('neutral', 1496), ('positive', 876)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAADuCAYAAAA0uwAcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIUSYkyAQQogwJ0EghBBhToJACCHC3P8HLxXW2LeMABMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae21d66ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#analysis attitude of all tweets by using the aboved two libraries\n",
    "import matplotlib.pyplot as plt\n",
    "#from TwitterPlot import generic_pie_plot\n",
    "%pylab inline\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_data(filename):\n",
    "    f = open(filename,'r', encoding='utf-8').read()\n",
    "    content_json = json.loads(f)\n",
    "    return content_json\n",
    "\n",
    "def estimate_features(data, negative_words, positive_words):\n",
    "    data_strings = []\n",
    "    for tweet in data:\n",
    "        if \"lang\" in tweet.keys() and tweet[\"lang\"]==\"en\" and \"text\" in tweet.keys() and len(tweet[\"text\"])!=0:\n",
    "            data_strings.append(tweet[\"text\"])\n",
    "    scores_map = {}\n",
    "    for sentence in data_strings:\n",
    "        score = 0\n",
    "        words_list = word_tokenize(sentence)\n",
    "        for word in words_list:\n",
    "            if word in negative_words:\n",
    "                score -= 1\n",
    "            if word in positive_words:\n",
    "                score += 1\n",
    "        scores_map[score] = scores_map.get(score, 0) + 1\n",
    "    sorted_scores = sorted(scores_map.items(), key=lambda t: t[0], reverse=True)\n",
    "    return sorted_scores\n",
    "\n",
    "def positive_negative_map(score_map):\n",
    "        p_n_map = {\"positive\":0, \"negative\":0, \"neutral\":0}\n",
    "        for pair in score_map:\n",
    "            if pair[0]>0:\n",
    "                p_n_map[\"positive\"] += pair[1]\n",
    "            elif pair[0] == 0:\n",
    "                p_n_map[\"neutral\"] += pair[1]\n",
    "            else:\n",
    "                p_n_map[\"negative\"] += pair[1]\n",
    "        generic_pie_plot(p_n_map, num_shown=3, include_others=False)\n",
    " \n",
    "negative_words = load_data(\"negative_words_cleaned.json\")\n",
    "positive_words = load_data(\"positive_words_cleaned.json\")\n",
    "data_strings = statuses\n",
    "\n",
    "score_map = estimate_features(data_strings, negative_words, positive_words)\n",
    "positive_negative_map(score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 6000\n",
      "[(None, 5840), ('Dublin City', 11), ('East', 5), ('London Stansted Airport - STN', 4), ('Sheffield', 4), ('Manchester', 3), ('Louth', 3), ('Lux', 3), ('Northern Ireland', 3), ('Camden Town', 2), ('South Dublin', 2), ('Stone', 2), ('Dorking', 2), ('Lewes', 2), ('Hetton', 2), ('Hilltown', 2), ('Brent', 2), ('Meath', 2), ('Edinburgh', 2), ('Blagnac', 2)]\n",
      "[(None, 5840), ('Dublin City', 11), ('East', 5), ('London Stansted Airport - STN', 4), ('Sheffield', 4), ('Manchester', 3), ('Louth', 3), ('Lux', 3), ('Northern Ireland', 3), ('Camden Town', 2)]\n",
      "[(None, 5840), ('United Kingdom', 72), ('Ireland', 26), ('France', 11), ('United States', 6), ('Italia', 5), ('Reino Unido', 5), ('Portugal', 4), ('Spain', 3), ('Germany', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Finding top 20 cities that tweets were sent from\n",
    "\n",
    "import json\n",
    "\n",
    "with open('ryanair_6000.txt') as f:\n",
    "    all_content = f.read()\n",
    "all_list = json.loads(all_content)\n",
    "print('len:', len(all_list))\n",
    "new_list = []\n",
    "for item in all_list:\n",
    "    new_dict = {}\n",
    "    for key in ['created_at', 'source', 'favorited', 'retweeted']:# 'quote_count', 'reply_count', 'retweet_count', 'favorite_count']:\n",
    "        new_dict[key] = item[key]\n",
    "    if item['place'] is not None:\n",
    "        #try:\n",
    "        new_dict['place_place_type'] = item['place']['place_type']\n",
    "        new_dict['place_city_name'] = item['place']['name']\n",
    "        new_dict['place_country'] = item['place']['country']\n",
    "        #except:\n",
    "        #    print(json.dumps(item, indent=4))\n",
    "        #    break\n",
    "    else:\n",
    "        new_dict['place_place_type'] = None\n",
    "        new_dict['place_city_name'] = None\n",
    "        new_dict['place_country'] = None\n",
    "        \n",
    "    if 'retweeted_status' in item:\n",
    "        for key in ['quote_count', 'reply_count', 'retweet_count', 'favorite_count']:\n",
    "            new_dict[key] = item['retweeted_status'][key]\n",
    "    else:\n",
    "        for key in ['quote_count', 'reply_count', 'retweet_count', 'favorite_count']:\n",
    "            new_dict[key] = -1\n",
    "        \n",
    "\n",
    "    new_dict['user_location'] = item['user']['location']\n",
    "    new_list.append(new_dict)\n",
    "\n",
    "\n",
    "# save\n",
    "with open('ryanair_6000_choose.txt', 'w') as f:\n",
    "    f.write(json.dumps(new_list, indent=4))\n",
    "\n",
    "# reopen and use\n",
    "with open('ryanair_6000_choose.txt') as f:\n",
    "    all_content = f.read()\n",
    "all_list = json.loads(all_content)\n",
    "\n",
    "all_create_at_list = [item['created_at'] for item in all_list]\n",
    "all_place_city = [item['place_city_name'] for item in all_list]\n",
    "all_place_country = [item['place_country'] for item in all_list]\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(all_place_city)\n",
    "print((c.most_common()[:20]))\n",
    "# loop use\n",
    "for item in [all_place_city, all_place_country, ]:\n",
    "    c = Counter(item)\n",
    "    print(c.most_common()[:10]) # top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Cities: \n",
      "\n",
      "+-------------------------------+-------+\n",
      "| City                          | Count |\n",
      "+-------------------------------+-------+\n",
      "| None                          |  5840 |\n",
      "| Dublin City                   |    11 |\n",
      "| East                          |     5 |\n",
      "| London Stansted Airport - STN |     4 |\n",
      "| Sheffield                     |     4 |\n",
      "| Manchester                    |     3 |\n",
      "| Louth                         |     3 |\n",
      "| Lux                           |     3 |\n",
      "| Northern Ireland              |     3 |\n",
      "| Camden Town                   |     2 |\n",
      "| South Dublin                  |     2 |\n",
      "| Stone                         |     2 |\n",
      "| Dorking                       |     2 |\n",
      "| Lewes                         |     2 |\n",
      "| Hetton                        |     2 |\n",
      "| Hilltown                      |     2 |\n",
      "| Brent                         |     2 |\n",
      "| Meath                         |     2 |\n",
      "| Edinburgh                     |     2 |\n",
      "| Blagnac                       |     2 |\n",
      "+-------------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.charts import Bar, show\n",
    "from prettytable import PrettyTable\n",
    "from collections import Counter\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Plot top 20 cities of twitter\n",
    "pt = PrettyTable(field_names=['City', 'Count'])\n",
    "city = Counter(all_place_city).most_common()\n",
    "[ pt.add_row(i) for i in city[:20] ]\n",
    "pt.align['City'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print(str(\"Top 20 Cities:\"),\"\\n\")\n",
    "print(pt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot top 10 countries of twitter\n",
    "country = [item['place_country'] for item in all_list]\n",
    "c = Counter(country)\n",
    "c_stat = c.most_common()[0:10]\n",
    "country_count_dict = {\n",
    "    'count': [item[1] for item in c.most_common()[0:10]],\n",
    "    'country': [item[0] for item in c_stat]\n",
    "}\n",
    "df_country = pd.DataFrame(country_count_dict)\n",
    "cp = Bar(df_country,'country',values = 'count', title = 'Distribution of Countries')\n",
    "cp.legend.location = None\n",
    "show(cp)\n",
    "#the output for piece of code opens up in a new tab and will show only when its run so we have included this graph \n",
    "#in the report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Yingnan Liu (yliu18@wpi.edu).\n",
    "\n",
    "#### We auto-process the submissions so make sure your subject line is *exactly*:\n",
    "\n",
    "### DS501 Case Study 1 Team ??\n",
    "\n",
    "#### where ?? is your team number.\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 120 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 80\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "  \n",
    "    -----------------------------------\n",
    "    Qestion 4:  Business question\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "        Novelty: 10\n",
    "        Interestingness: 10\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
